{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 西瓜书\n",
    "## 线性模型(linear model)\n",
    "### 线性模型基本形式\n",
    "给定一组数据 $ {X}=\\{ {x}_1, {x}_2,..., {x}_i\\}$,其中每个$ {x}_i$有$ {d}$个属性即$ {x}_i=\\{ {x}_i^{(1)}, {x}_i^{(2)},..., {x}_i^{(d)}\\}$,线性模型(linear model)试图通过属性的线性组合还进行预测的函数,即\n",
    "$$ {f}( {x})= {w}_i^{1} {x}_i^{(1)}+ {w}_i^{2} {x}_i^{(2)}+...+ {w}_i^{d} {x}_i^{(d)}+b_i$$,\n",
    "用一般向量的形式为:\n",
    "$$ {f}( {x})= {w}_i^T {x}_i+b_i,$$\n",
    "其中$ {w}_i=( {w}_i^{1}, {w}_i^{2},..., {w}_i^{d})$.$ {w}_i$和b学的之后,模型得到确定."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归\n",
    "从上面的描述中我们有一组数据,但我们现在只用了其中一组数据表示线性模型,线性回归则是使用全部训练数据来构建误差函数,但我们还缺少一组观测值$ {Y}=\\{ {y}_1, {y}_2, {y}_3,..., {y}_i\\}$,这样我们有了线性模型和观测值$ {Y}$,并将$w,b$扩展成一个大矩阵$W$我们就可以构建误差函数:\n",
    "$$ {F} (  {X} )=\\operatorname*{argmin}_{W} 0.5 \\sum \\{ \\hat{f}-Y\\}^2$$,\n",
    "这样我们就可以使用\"最小二乘法\"来求解.\n",
    "对$W$进行求导得到:\n",
    "$$\\frac{\\theta F}{\\theta W}=X^T(XW-Y)$$\n",
    "然后令上式为零即可得w和b的闭式解(closed-form solve)\n",
    "当我们的数据集$X^TX$矩阵满秩或者正定时:\n",
    "$$W=(X^TX)^{-1}X^TY,$$\n",
    "更一般的我们可以把模型的预测逼近到y的衍生物,即所谓广义线性回归.\n",
    "$$y=g^{-1}(w^Tx+b)$$\n",
    "这样得到的模型称为广义线性模型,其中$g()$称为联系函数."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数几率回归\n",
    "对数几率回归即,联系函数$g()$为\n",
    "$$g(y)=\\frac{1}{1-e^{-y}}$$\n",
    "其中$y$为预测,\n",
    "对数几率函数其实是对单位阶跃函数的替代函数,因为其具有连续可微,并且在y=0附近有突变,与单位阶跃函数很接近.\n",
    "我们将其写成如下形式:\n",
    "$$ln\\frac{y}{1-y}=WX$$\n",
    "可以看出上式左边是一个二分类问题的概率比值.于是我们可以将其表示为:\n",
    "$$ln\\frac{p(y=1|X)}{p(y=0|X)}$$\n",
    "$$p(y=1|X)=\\frac{e^{W^TX}}{1+e^{W^TX}}$$\n",
    "$$p(y=0|X)=\\frac{1}{1+e^{W^TX}}$$\n",
    "我们可以使用极大似然法去求解这个问题,即对给定数据集使得:\n",
    "$$\\ell{(W)}=\\sum lnp(y_i|x_i;W)$$\n",
    "最大化.其中$p(y_i|x_i;W)$可以写成:\n",
    "$$y_ip(y=1|x_i;W)+(1-y_i)p(y=0|x_i;W)$$\n",
    "将上面几个式子联立可得最大化似然估计等价于最小化下式,即:\n",
    "$$\\ell{(W)}=min\\sum (-y_iW^Tx_i+ln(1+e^{W^Tx_i}))$$\n",
    "于是我们可以使用数值优化的方法去求解$W$.\n",
    "由此我们可以使用线性回归去逼近一个分类问题."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性判别分析\n",
    "简称LDA(Linear Discrimination Analysis)是一种典型的线性学习方法.\n",
    "LDA思想很朴素:给定训练样本集,设法将样本例投影到一条直线上,使得同类样例的投影点尽可能的接近,异类样例投影点尽可能的远离;新的样本将其投影到同一条直线上,根据其投影点的位置判断其类别.\n",
    "给定一组数据集$D=\\{x_i,y_i\\}_{i=1}^m$,$y_i\\in \\{0,1\\}$,令$X_i,\\mu_i,\\Sigma_i$分别表示第$i\\in\\{0,1\\}$类示例的集合,均值向量和协方差矩阵.将数据集投影到直线上,两个样本集的中心分别为$w^T\\mu_0,w^T\\mu_1$;投影点的协方差分别为$w^T\\Sigma_0w,w^T\\Sigma_1w$由于只显示一维空间,所以上述四个量均为是实数.\n",
    "可以想象欲使数据集更好的分类,相同类型数据的投影点应尽可能的接近即协方差$w^T\\Sigma_0w+w^T\\Sigma_1w$应尽可能的小;同样不同类型应相距较远即$||w^T\\mu_0-w^T\\mu_1||_2^2$应尽可能的大,同时考虑二者则可得目标函数:\n",
    "$$J=\\frac{||w^T\\mu_0-w^T\\mu_1||_2^2}{w^T\\Sigma_0w+w^T\\Sigma_1w}$$\n",
    "$$=\\frac{w^T(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^Tw}{w^T(\\Sigma _0+\\Sigma_1)w}$$\n",
    "令$(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T=S_w$称为类内散度\n",
    "令$(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T=S_b$称为类间散度\n",
    "从上式可以看出目标函数的解与与$w$的长度无关,只和其方向有关,此问题可以等价于:\n",
    "$$\\operatorname*{min}_w \\ \\ \\ -w^TS_bw$$\n",
    "$$s.t.\\ \\ \\ w^TS_ww=1$$\n",
    "由拉格朗日乘子法,上式等价于:\n",
    "$$S_bw=\\lambda S_ww$$\n",
    "注意到$S_bw$的方向恒为$\\mu_0-\\mu_1$,不妨令\n",
    "$$S_bw=\\lambda(\\mu_0-\\mu_1) $$\n",
    "联立几个等式可以得到:\n",
    "$$w=S_w^{-1}(\\mu_0-\\mu_1)$$\n",
    "在矩阵求逆的过程中考虑到数值解的稳定性通常使用SVD求逆.\n",
    "同样LDA可以推广到多分类任务中,其思想与二分类类似:\n",
    "将N各类的数据投影到N-1为的空间中,目标函数为:\n",
    "$$\\operatorname*{max}_W\\frac{tr(W^TS_bW)}{tr(W^TS_wW)}$$\n",
    "定义全局散度矩阵:\n",
    "$$S_t=S_b+S_w\\\\=\\sum_{i=1}^{m}(x_i-\\mu)(x_i-\\mu)^T$$\n",
    "其中$\\mu$为所有数据的均值向量.$S_w$为所有类别散度矩阵之和:\n",
    "$$S_w=\\sum_{i=1}^{N}S_{w_i}$$\n",
    "有上述两式可已得到$S_b$\n",
    "可以看出多分类LDA是一种经典的监督降维技术;因为其使用W投影矩阵将数据集投影到了N-1维空间,同时又使用了类别信息."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  多分类学习\n",
    "考虑N个类别$C_1,C_2,C_3,...,C_N$,多分类问题可以分解成若干个二分类任务求解.具体来说,先对问题进行拆分,然后为拆出的每个二分类问题训练一个分类器,然后对测试结果进行集成已获得最终的多分类结果.\n",
    "经典的拆分策略:\"一对一\"(OvO),\"一对其余\"(OvR),\"多对多\"(MvM).\n",
    "OvO即对N个类别进行两两配对,从而产生$N(N-1)/2$个二分类任务,训练时每个二分类任务之分得其正反两个类型的数据其他数据在此任务中舍去.测试则是对所有分类器进行提交,通过投票得到最终结果.\n",
    "OvR则是将N个类别分成N个二分类任务,每个任务由其中一类和其余类组成.测试时若只有一个类别为正类,则作为最总结果,如果产生多个类别正类,则考虑分类器置信度.\n",
    "MvM则需要对正反类的结构进行特殊设计,不能随便选取.常用的技术:纠错输出码(ECOC)\n",
    "ECOC是将编码的思想引入类别拆分,并尽可能在解码过程中具有容错性.工作过程主要分为两步:\n",
    "1.编码:对N个类别做M次划分,每次划分分成两部分;这样共产生M个训练集,可训练出M个分类器.\n",
    "2.解码:M个分类器分别对测试样本进行预测,这些预测表及组成一个编码.将这个预测编码与每个类别各自的编码进行比较,返回其中距离最小的类别作为最终结果."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]Methods for non-linear least squares problems. K. Madsen, H.B. Nielsen, O. Tingleff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
